{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification des documents du procès des groupes américains du tabac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ilyas\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# libraries for dataset preparation, feature engineering, model training \n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import preprocessing, linear_model, svm\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import ast\n",
    "import os\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import GRU, Dropout, MaxPooling1D, Conv1D, Flatten, BatchNormalization, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import (classification_report, \n",
    "                             precision_recall_fscore_support, \n",
    "                             accuracy_score)\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preper(PATH_data, del_saut_line):\n",
    "    \n",
    "    \"\"\" Inputs : * PATH_data --> type str, chemin du dossier contenant les 10 dossiers de donnees.\n",
    "                 * del_saut_line --> type bool, si True, on récupère nos donnees texte sans le saut de ligne '\\n'.\n",
    "        Outputs: * Data frame, Contenant deux colonnes, une correspond au textes, et l'autre au labels. \"\"\"\n",
    "    \n",
    "    classes = []\n",
    "    filenames = []\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # Parcours du dossier contenant les 10 dossiers correspondants au classes\n",
    "    for i, classe in enumerate (os.listdir(PATH_data)):\n",
    "        \n",
    "        inputFilepath = classe\n",
    "        filename_w_ext = os.path.basename(inputFilepath)\n",
    "        filename, file_extension = os.path.splitext(filename_w_ext)\n",
    "        classes.append(filename)\n",
    "        path = PATH_data + '/' + filename\n",
    "\n",
    "        # Parcours de chaque dossier pour extraire le texte de chaque fichier\n",
    "        for j, element in enumerate (os.listdir(path)):\n",
    "            inputFilepath = element\n",
    "            filename_w_ext = os.path.basename(inputFilepath)\n",
    "            filename, file_extension = os.path.splitext(filename_w_ext)\n",
    "            filenames.append(filename)\n",
    "            path_file = path + '/' + element\n",
    "            \n",
    "            file = open(path_file,'r',encoding='utf8').read()\n",
    "            if del_saut_line == True:\n",
    "                text = \"\"\n",
    "                for k, line in enumerate(file.split(\"\\n\")):\n",
    "                    content = line.split()\n",
    "                    text += (\" \".join(content[1:]))\n",
    "                X.append(text)\n",
    "                y.append(classes[i])\n",
    "                \n",
    "            if del_saut_line == False:\n",
    "                X.append(file)\n",
    "                y.append(classes[i])\n",
    "\n",
    "    y = np.asarray(y)\n",
    "    X = np.asarray(X)\n",
    "    \n",
    "    # create a dataframe using texts and lables\n",
    "    df = pd.DataFrame()\n",
    "    df['text'] = X\n",
    "    df['label'] = y\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nombre de donnees:  3482\n",
      "\n",
      "Exemple du dataframe avec saut de ligne:\n",
      "                                                   text          label\n",
      "220  ce\\n\\n  \\n\\nNew: Laser-Cut “Flavor Chamber”\\n\\...  Advertisement\n",
      "221  THE IMAGINATION To Sez It,\\nTHE Passion To Pur...  Advertisement\n",
      "222  NIORE\\nPACKINGS.\\n\\nIIORE\\nPLEASURE.\\n\\nee\\nNe...  Advertisement\n",
      "223  SURGEON GENERAL'S WARNING: Smoking\\nCauses Lun...  Advertisement\n",
      "224  SURGEON GENERAL'S WARNING: Cigarette\\nSmoke Co...  Advertisement\n",
      "225  Helping Youth Say No” is only one part of wide...  Advertisement\n",
      "226  Senge,\\n\\n~  SOYOUDONT |\\nSMOKE!\\n\\n \\n\\n~ .\\n...  Advertisement\n",
      "227  The tobacco industry is committed to seeing th...  Advertisement\n",
      "228  S\\n2\\n\\ns\\n\\n \\n\\n~HOLD FOR OUR INDUSTRY?\\n\\n»...  Advertisement\n",
      "229  Yon CE MOTE PSU AQ YN IS MLE\\n\\nALALILSNI ODDV...  Advertisement\n",
      "230  Ellis, Cathy L. (WSA)\\n\\n \\n\\nFrom: Hsu, Frank...          Email\n",
      "231  Pabst, Joanne\\n\\n \\n\\nFrom: Keane, Denise\\n\\nS...          Email\n",
      "232  — Original Message—-—\\n\\nFrom: Berlind, Mark\\n...          Email\n",
      "233  From: — Greenberg, David\\n\\nSent: Saturday, Ju...          Email\n",
      "234  From: Waadward, Ettis\\n\\nSent: Tuesday, June 3...          Email\n",
      "235  —~-Original Message———\\n\\nFrom: Osborne, Kevin...          Email\n",
      "236  From: Atkins, Sharon on Tue, Apr 16, 1996 3:52...          Email\n",
      "237  R: REDACTED MATERIAL\\n\\n \\n\\nMurillo, Joe\\n\\nF...          Email\n",
      "238  Walk, Roger A.\\n\\nFrom: Walk, Roger A.\\n\\nSent...          Email\n",
      "239  Galan, Nancy\\n\\n@-: Suter, Norma\\nSent: Monday...          Email\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preparation des donnees sans enlever '/n'\n",
    "\n",
    "PATH_data = 'C:/Users/Ilyas/Desktop/M2 SID - SD/Text Analysis/nlp-labs/tobacco-lab/data'\n",
    "df = data_preper(PATH_data, del_saut_line=False)\n",
    "\n",
    "sns.countplot(data=df,y='label')\n",
    "\n",
    "print('\\nNombre de donnees: ',df['text'].shape[0])\n",
    "print('\\nExemple du dataframe avec saut de ligne:\\n',df[220:240])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nombre de donnees:  3482\n",
      "X_train:  (2819,)\n",
      "X_test:  (349,)\n",
      "X_val:  (314,)\n"
     ]
    }
   ],
   "source": [
    "X_app, X_test, y_app, y_test = train_test_split(df['text'], df['label'], test_size=0.1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_app, y_app, test_size=0.1)\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.fit_transform(y_test)\n",
    "y_val = encoder.fit_transform(y_val)\n",
    "y_app = encoder.fit_transform(y_app)\n",
    "\n",
    "print('\\nNombre de donnees: ',df['text'].shape[0])\n",
    "print('X_train: ',X_train.shape)\n",
    "print('X_test: ',X_test.shape)\n",
    "print('X_val: ',X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count Vectors as features : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document vectors\n",
    "vectorizer = CountVectorizer(max_features=3000,analyzer='word', token_pattern=r'\\w{1,}')\n",
    "vectorizer.fit(df['text'])\n",
    "\n",
    "X_train_counts = vectorizer.transform(X_train)\n",
    "X_val_counts = vectorizer.transform(X_val)\n",
    "X_test_counts = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-idf Vectors as features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram level tf-idf:\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,4), max_features=1000)\n",
    "tfidf_vect.fit(df['text'])\n",
    "\n",
    "X_train_tf =  tfidf_vect.transform(X_train)\n",
    "X_val_tf =  tfidf_vect.transform(X_val)\n",
    "X_test_tf = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With Count Vectors representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Validation du classifieur: \n",
      "\n",
      "Score :  0.7356687898089171\n",
      "\n",
      " - Prediction des classes et test sur les donnees de test: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.70      0.70        23\n",
      "          1       0.98      0.94      0.96        63\n",
      "          2       0.74      0.90      0.81        48\n",
      "          3       0.81      0.68      0.74        65\n",
      "          4       0.66      0.78      0.71        51\n",
      "          5       0.71      0.83      0.77        18\n",
      "          6       0.38      0.33      0.36        15\n",
      "          7       0.60      0.46      0.52        26\n",
      "          8       1.00      1.00      1.00        15\n",
      "          9       0.67      0.64      0.65        25\n",
      "\n",
      "avg / total       0.76      0.76      0.76       349\n",
      "\n",
      "matrice de confusion : \n",
      " [[16  0  0  0  2  0  4  1  0  0]\n",
      " [ 0 59  0  1  2  1  0  0  0  0]\n",
      " [ 2  0 43  1  0  0  1  0  0  1]\n",
      " [ 0  1  5 44  5  4  1  4  0  1]\n",
      " [ 0  0  2  6 40  0  1  2  0  0]\n",
      " [ 2  0  0  0  0 15  1  0  0  0]\n",
      " [ 2  0  3  0  5  0  5  0  0  0]\n",
      " [ 0  0  1  2  4  1  0 12  0  6]\n",
      " [ 0  0  0  0  0  0  0  0 15  0]\n",
      " [ 1  0  4  0  3  0  0  1  0 16]]\n"
     ]
    }
   ],
   "source": [
    "# train a Naive Bayes classifier:\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_counts,y_train)\n",
    "score_val = clf.score(X_val_counts,y_val)\n",
    "print(' - Validation du classifieur: \\n')\n",
    "print('Score : ',score_val)\n",
    "\n",
    "# predict test classes:\n",
    "print('\\n - Prediction des classes et test sur les donnees de test: \\n')\n",
    "y_pred = clf.predict(X_test_counts)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('matrice de confusion : \\n',confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With TF-IDF Vectors representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score :  0.7261146496815286\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.70      0.76        23\n",
      "          1       0.97      0.95      0.96        63\n",
      "          2       0.65      0.96      0.77        48\n",
      "          3       0.80      0.74      0.77        65\n",
      "          4       0.64      0.80      0.71        51\n",
      "          5       0.79      0.83      0.81        18\n",
      "          6       1.00      0.07      0.12        15\n",
      "          7       0.58      0.27      0.37        26\n",
      "          8       1.00      1.00      1.00        15\n",
      "          9       0.65      0.68      0.67        25\n",
      "\n",
      "avg / total       0.78      0.76      0.74       349\n",
      "\n",
      "matrice de confusion : \n",
      " [[16  0  3  0  4  0  0  0  0  0]\n",
      " [ 0 60  0  1  1  1  0  0  0  0]\n",
      " [ 1  0 46  1  0  0  0  0  0  0]\n",
      " [ 0  1  4 48  6  0  0  3  0  3]\n",
      " [ 0  0  3  6 41  0  0  1  0  0]\n",
      " [ 2  0  1  0  0 15  0  0  0  0]\n",
      " [ 0  0  8  1  5  0  1  0  0  0]\n",
      " [ 0  1  1  3  5  3  0  7  0  6]\n",
      " [ 0  0  0  0  0  0  0  0 15  0]\n",
      " [ 0  0  5  0  2  0  0  1  0 17]]\n"
     ]
    }
   ],
   "source": [
    "# train a Naive Bayes classifier:\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_tf,y_train)\n",
    "score_val = clf.score(X_val_tf,y_val)\n",
    "print('score : ',score_val)\n",
    "\n",
    "# predict test classes:\n",
    "y_pred = clf.predict(X_test_tf)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('matrice de confusion : \\n',confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With Count Vectors representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Validation du classifieur: \n",
      "\n",
      "Score :  0.7929936305732485\n",
      "\n",
      " - Prediction des classes et test sur les donnees de test: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.65      0.71        23\n",
      "          1       0.94      0.98      0.96        63\n",
      "          2       0.83      0.92      0.87        48\n",
      "          3       0.84      0.78      0.81        65\n",
      "          4       0.75      0.84      0.80        51\n",
      "          5       0.88      0.83      0.86        18\n",
      "          6       0.57      0.80      0.67        15\n",
      "          7       0.75      0.46      0.57        26\n",
      "          8       1.00      1.00      1.00        15\n",
      "          9       0.67      0.64      0.65        25\n",
      "\n",
      "avg / total       0.82      0.82      0.81       349\n",
      "\n",
      "matrice de confusion : \n",
      " [[15  1  1  0  1  0  5  0  0  0]\n",
      " [ 0 62  0  1  0  0  0  0  0  0]\n",
      " [ 1  0 44  1  0  0  2  0  0  0]\n",
      " [ 1  1  1 51  7  1  0  1  0  2]\n",
      " [ 1  0  0  6 43  0  1  0  0  0]\n",
      " [ 1  1  0  0  0 15  0  0  0  1]\n",
      " [ 0  0  2  0  1  0 12  0  0  0]\n",
      " [ 0  1  2  2  3  1  0 12  0  5]\n",
      " [ 0  0  0  0  0  0  0  0 15  0]\n",
      " [ 0  0  3  0  2  0  1  3  0 16]]\n"
     ]
    }
   ],
   "source": [
    "# train a Linear classifier:\n",
    "lc = linear_model.LogisticRegression()\n",
    "lc.fit(X_train_counts,y_train)\n",
    "score_val = lc.score(X_val_counts,y_val)\n",
    "print(' - Validation du classifieur: \\n')\n",
    "print('Score : ',score_val)\n",
    "\n",
    "# predict test classes:\n",
    "print('\\n - Prediction des classes et test sur les donnees de test: \\n')\n",
    "y_pred = lc.predict(X_test_counts)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('matrice de confusion : \\n',confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With TF-IDF Vectors representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score :  0.7738853503184714\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.57      0.65        23\n",
      "          1       0.95      0.95      0.95        63\n",
      "          2       0.72      0.88      0.79        48\n",
      "          3       0.87      0.74      0.80        65\n",
      "          4       0.72      0.84      0.77        51\n",
      "          5       0.76      0.89      0.82        18\n",
      "          6       0.56      0.67      0.61        15\n",
      "          7       0.64      0.35      0.45        26\n",
      "          8       1.00      1.00      1.00        15\n",
      "          9       0.61      0.68      0.64        25\n",
      "\n",
      "avg / total       0.79      0.78      0.78       349\n",
      "\n",
      "matrice de confusion : \n",
      " [[13  0  2  0  3  1  4  0  0  0]\n",
      " [ 0 60  0  1  1  1  0  0  0  0]\n",
      " [ 2  0 42  0  0  0  3  0  0  1]\n",
      " [ 1  1  4 48  5  1  0  3  0  2]\n",
      " [ 0  1  1  5 43  0  0  1  0  0]\n",
      " [ 1  0  1  0  0 16  0  0  0  0]\n",
      " [ 0  0  3  0  2  0 10  0  0  0]\n",
      " [ 0  1  1  1  4  2  0  9  0  8]\n",
      " [ 0  0  0  0  0  0  0  0 15  0]\n",
      " [ 0  0  4  0  2  0  1  1  0 17]]\n"
     ]
    }
   ],
   "source": [
    "# train a Linear classifier:\n",
    "lc = linear_model.LogisticRegression()\n",
    "lc.fit(X_train_tf,y_train)\n",
    "score_val = lc.score(X_val_tf,y_val)\n",
    "print('score : ',score_val)\n",
    "\n",
    "# predict test classes:\n",
    "y_pred = lc.predict(X_test_tf)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('matrice de confusion : \\n',confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bagging Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score :  0.7420382165605095\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.70      0.70        23\n",
      "          1       0.91      0.97      0.94        63\n",
      "          2       0.65      0.83      0.73        48\n",
      "          3       0.80      0.78      0.79        65\n",
      "          4       0.81      0.76      0.79        51\n",
      "          5       0.82      0.78      0.80        18\n",
      "          6       0.57      0.53      0.55        15\n",
      "          7       0.44      0.27      0.33        26\n",
      "          8       1.00      1.00      1.00        15\n",
      "          9       0.65      0.60      0.63        25\n",
      "\n",
      "avg / total       0.76      0.76      0.76       349\n",
      "\n",
      "matrice de confusion : \n",
      " [[16  0  2  1  0  0  3  1  0  0]\n",
      " [ 0 61  0  1  0  0  0  1  0  0]\n",
      " [ 1  2 40  0  1  0  1  2  0  1]\n",
      " [ 0  1  4 51  5  0  1  2  0  1]\n",
      " [ 1  2  2  6 39  0  0  1  0  0]\n",
      " [ 2  0  2  0  0 14  0  0  0  0]\n",
      " [ 2  1  3  1  0  0  8  0  0  0]\n",
      " [ 0  0  5  3  2  3  0  7  0  6]\n",
      " [ 0  0  0  0  0  0  0  0 15  0]\n",
      " [ 1  0  4  1  1  0  1  2  0 15]]\n"
     ]
    }
   ],
   "source": [
    "# train a bagging model:\n",
    "clf = tree.DecisionTreeClassifier(max_depth=100)      # 100 arbres\n",
    "bagging = BaggingClassifier(clf,max_samples=0.5, max_features=0.5)\n",
    "bagging.fit(X_train_tf,y_train)\n",
    "score_val = bagging.score(X_val_tf,y_val)\n",
    "print('score : ',score_val)\n",
    "\n",
    "# predict test classes:\n",
    "y_pred = bagging.predict(X_test_tf)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('matrice de confusion : \\n',confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reseau de neuronnes convoltionnel (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(train_raw_text, test_raw_text):\n",
    "    \n",
    "    tokenizer = text.Tokenizer(num_words=MAX_FEATURES, )\n",
    "\n",
    "    tokenizer.fit_on_texts(list(train_raw_text))\n",
    "    train_tokenized = tokenizer.texts_to_sequences(train_raw_text)\n",
    "    test_tokenized = tokenizer.texts_to_sequences(test_raw_text)\n",
    "    return sequence.pad_sequences(train_tokenized, maxlen=MAX_TEXT_LENGTH), \\\n",
    "           sequence.pad_sequences(test_tokenized, maxlen=MAX_TEXT_LENGTH)\n",
    "\n",
    "def get_model():\n",
    "\n",
    "    inp = Input(shape=(MAX_TEXT_LENGTH,))\n",
    "    model = Embedding(MAX_TEXT_LENGTH, EMBED_SIZE)(inp)\n",
    "    \n",
    "    model = Conv1D(filters=64, kernel_size=7, padding='same', activation='relu')(model)\n",
    "    model = MaxPooling1D(pool_size=3)(model)\n",
    "    model = BatchNormalization(axis=1)(model)\n",
    "    model = Dropout(0.25)(model)\n",
    "    \n",
    "    model = Conv1D(filters=128, kernel_size=5, padding='same', activation='relu')(model)\n",
    "    model = MaxPooling1D(pool_size=5)(model)\n",
    "    model = BatchNormalization(axis=1)(model)\n",
    "    model = Dropout(0.3)(model)\n",
    "    \n",
    "    model = Flatten()(model)\n",
    "    model = Dense(1024, activation=\"relu\")(model)\n",
    "    model = Dense(10, activation=\"softmax\")(model)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=model)\n",
    "    \n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "3133 349\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 500, 32)           16000     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 64)           14400     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 166, 64)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 166, 64)           664       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 166, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 166, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 33, 128)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 33, 128)           132       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 33, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4224)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              4326400   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 4,408,934\n",
      "Trainable params: 4,408,536\n",
      "Non-trainable params: 398\n",
      "_________________________________________________________________\n",
      "Train on 2819 samples, validate on 314 samples\n",
      "Epoch 1/50\n",
      "2819/2819 [==============================] - 19s 7ms/step - loss: 12.7370 - acc: 0.1674 - val_loss: 11.2424 - val_acc: 0.2898\n",
      "Epoch 2/50\n",
      "2819/2819 [==============================] - 21s 7ms/step - loss: 7.9871 - acc: 0.2685 - val_loss: 3.8670 - val_acc: 0.2834\n",
      "Epoch 3/50\n",
      "2819/2819 [==============================] - 21s 8ms/step - loss: 2.6224 - acc: 0.2909 - val_loss: 2.7587 - val_acc: 0.2675\n",
      "Epoch 4/50\n",
      "2819/2819 [==============================] - 19s 7ms/step - loss: 2.0223 - acc: 0.3590 - val_loss: 2.3073 - val_acc: 0.3121\n",
      "Epoch 5/50\n",
      "2819/2819 [==============================] - 20s 7ms/step - loss: 1.5026 - acc: 0.4732 - val_loss: 1.5042 - val_acc: 0.4904\n",
      "Epoch 6/50\n",
      "2819/2819 [==============================] - 20s 7ms/step - loss: 1.2348 - acc: 0.5502 - val_loss: 1.5838 - val_acc: 0.4076\n",
      "Epoch 7/50\n",
      "2819/2819 [==============================] - 21s 7ms/step - loss: 1.0549 - acc: 0.6233 - val_loss: 1.3209 - val_acc: 0.4936\n",
      "Epoch 8/50\n",
      "2819/2819 [==============================] - 19s 7ms/step - loss: 0.8843 - acc: 0.6790 - val_loss: 1.0863 - val_acc: 0.6083\n",
      "Epoch 9/50\n",
      "2819/2819 [==============================] - 19s 7ms/step - loss: 0.7374 - acc: 0.7272 - val_loss: 1.7652 - val_acc: 0.5669\n",
      "Epoch 10/50\n",
      "2819/2819 [==============================] - 19s 7ms/step - loss: 0.6401 - acc: 0.7716 - val_loss: 1.0171 - val_acc: 0.6178\n",
      "Epoch 11/50\n",
      "2819/2819 [==============================] - 19s 7ms/step - loss: 0.5248 - acc: 0.8134 - val_loss: 1.5766 - val_acc: 0.6210\n",
      "Epoch 12/50\n",
      "2819/2819 [==============================] - 19s 7ms/step - loss: 0.4359 - acc: 0.8460 - val_loss: 1.1934 - val_acc: 0.7006\n",
      "Epoch 13/50\n",
      "2819/2819 [==============================] - 20s 7ms/step - loss: 0.3991 - acc: 0.8609 - val_loss: 1.4547 - val_acc: 0.6592\n",
      "Epoch 14/50\n",
      "2819/2819 [==============================] - 18s 6ms/step - loss: 0.3333 - acc: 0.8879 - val_loss: 1.5479 - val_acc: 0.5796\n",
      "Epoch 15/50\n",
      "2819/2819 [==============================] - 18s 6ms/step - loss: 0.2645 - acc: 0.8968 - val_loss: 1.6994 - val_acc: 0.6688\n",
      "Epoch 16/50\n",
      "2819/2819 [==============================] - 25s 9ms/step - loss: 0.2604 - acc: 0.9085 - val_loss: 1.2097 - val_acc: 0.6752\n",
      "Epoch 17/50\n",
      "2819/2819 [==============================] - 27s 10ms/step - loss: 0.2066 - acc: 0.9227 - val_loss: 1.2070 - val_acc: 0.7006\n",
      "Epoch 18/50\n",
      "2819/2819 [==============================] - 28s 10ms/step - loss: 0.2128 - acc: 0.9198 - val_loss: 1.1642 - val_acc: 0.7261\n",
      "Epoch 19/50\n",
      "2819/2819 [==============================] - 28s 10ms/step - loss: 0.1623 - acc: 0.9457 - val_loss: 1.7267 - val_acc: 0.6815\n",
      "Epoch 20/50\n",
      "2819/2819 [==============================] - 27s 10ms/step - loss: 0.2130 - acc: 0.9379 - val_loss: 1.8580 - val_acc: 0.6529\n",
      "Epoch 21/50\n",
      "2819/2819 [==============================] - 26s 9ms/step - loss: 0.1722 - acc: 0.9475 - val_loss: 1.0863 - val_acc: 0.7580\n",
      "Epoch 22/50\n",
      "2819/2819 [==============================] - 27s 10ms/step - loss: 0.1471 - acc: 0.9549 - val_loss: 1.2987 - val_acc: 0.7293\n",
      "Epoch 23/50\n",
      "2819/2819 [==============================] - 27s 9ms/step - loss: 0.1466 - acc: 0.9546 - val_loss: 2.7589 - val_acc: 0.6529\n",
      "Epoch 24/50\n",
      "2819/2819 [==============================] - 26s 9ms/step - loss: 0.1202 - acc: 0.9599 - val_loss: 1.3122 - val_acc: 0.7261\n",
      "Epoch 25/50\n",
      "2819/2819 [==============================] - 27s 10ms/step - loss: 0.1150 - acc: 0.9635 - val_loss: 2.1988 - val_acc: 0.7070\n",
      "Epoch 26/50\n",
      "2819/2819 [==============================] - 27s 10ms/step - loss: 0.1049 - acc: 0.9649 - val_loss: 1.8344 - val_acc: 0.7166\n",
      "Epoch 27/50\n",
      "2819/2819 [==============================] - 27s 10ms/step - loss: 0.1161 - acc: 0.9691 - val_loss: 2.1927 - val_acc: 0.6847\n",
      "Epoch 28/50\n",
      "2819/2819 [==============================] - 27s 10ms/step - loss: 0.1143 - acc: 0.9656 - val_loss: 1.5414 - val_acc: 0.7325\n",
      "Epoch 29/50\n",
      "2819/2819 [==============================] - 26s 9ms/step - loss: 0.1001 - acc: 0.9674 - val_loss: 1.9078 - val_acc: 0.7166\n",
      "Epoch 30/50\n",
      "2819/2819 [==============================] - 26s 9ms/step - loss: 0.1111 - acc: 0.9702 - val_loss: 1.9485 - val_acc: 0.7166\n",
      "Epoch 31/50\n",
      "2819/2819 [==============================] - 26s 9ms/step - loss: 0.0905 - acc: 0.9713 - val_loss: 1.4562 - val_acc: 0.7357\n",
      "Epoch 32/50\n",
      "2819/2819 [==============================] - 26s 9ms/step - loss: 0.0862 - acc: 0.9745 - val_loss: 1.3499 - val_acc: 0.7548\n",
      "Epoch 33/50\n",
      "2819/2819 [==============================] - 27s 9ms/step - loss: 0.0585 - acc: 0.9777 - val_loss: 1.9393 - val_acc: 0.7293\n",
      "Epoch 34/50\n",
      "2819/2819 [==============================] - 26s 9ms/step - loss: 0.0752 - acc: 0.9752 - val_loss: 1.4952 - val_acc: 0.7580\n",
      "Epoch 35/50\n",
      "2819/2819 [==============================] - 26s 9ms/step - loss: 0.0863 - acc: 0.9748 - val_loss: 1.5879 - val_acc: 0.7389\n",
      "Epoch 36/50\n",
      "2819/2819 [==============================] - 26s 9ms/step - loss: 0.1242 - acc: 0.9698 - val_loss: 1.7490 - val_acc: 0.7516\n",
      "Epoch 37/50\n",
      "2819/2819 [==============================] - 28s 10ms/step - loss: 0.0671 - acc: 0.9794 - val_loss: 1.4796 - val_acc: 0.7580\n",
      "Epoch 38/50\n",
      "2819/2819 [==============================] - 25s 9ms/step - loss: 0.1004 - acc: 0.9706 - val_loss: 2.2094 - val_acc: 0.7166\n",
      "Epoch 39/50\n",
      "2819/2819 [==============================] - 26s 9ms/step - loss: 0.0977 - acc: 0.9727 - val_loss: 1.8178 - val_acc: 0.7325\n",
      "Epoch 40/50\n",
      "2819/2819 [==============================] - 26s 9ms/step - loss: 0.0615 - acc: 0.9808 - val_loss: 1.7913 - val_acc: 0.7357\n",
      "Epoch 41/50\n",
      "2819/2819 [==============================] - 27s 10ms/step - loss: 0.0957 - acc: 0.9723 - val_loss: 1.8274 - val_acc: 0.7166\n",
      "Epoch 42/50\n",
      "2819/2819 [==============================] - 26s 9ms/step - loss: 0.0833 - acc: 0.9787 - val_loss: 1.5715 - val_acc: 0.7197\n",
      "Epoch 43/50\n",
      "2819/2819 [==============================] - 27s 9ms/step - loss: 0.0673 - acc: 0.9794 - val_loss: 1.9432 - val_acc: 0.7229\n",
      "Epoch 44/50\n",
      "2819/2819 [==============================] - 26s 9ms/step - loss: 0.0765 - acc: 0.9787 - val_loss: 1.8541 - val_acc: 0.7293\n",
      "Epoch 45/50\n",
      "2819/2819 [==============================] - 26s 9ms/step - loss: 0.0513 - acc: 0.9826 - val_loss: 1.9287 - val_acc: 0.7166\n",
      "Epoch 46/50\n",
      "2819/2819 [==============================] - 25s 9ms/step - loss: 0.0708 - acc: 0.9777 - val_loss: 1.8456 - val_acc: 0.7420\n",
      "Epoch 47/50\n",
      "2819/2819 [==============================] - 26s 9ms/step - loss: 0.0530 - acc: 0.9801 - val_loss: 1.8548 - val_acc: 0.7516\n",
      "Epoch 48/50\n",
      "2819/2819 [==============================] - 27s 10ms/step - loss: 0.0567 - acc: 0.9787 - val_loss: 1.9256 - val_acc: 0.7134\n",
      "Epoch 49/50\n",
      "2819/2819 [==============================] - 26s 9ms/step - loss: 0.0944 - acc: 0.9777 - val_loss: 2.1906 - val_acc: 0.6911\n",
      "Epoch 50/50\n",
      "2819/2819 [==============================] - 28s 10ms/step - loss: 0.0434 - acc: 0.9833 - val_loss: 1.8796 - val_acc: 0.7484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f118c3b828>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model parameters\n",
    "\n",
    "MAX_FEATURES = 500\n",
    "MAX_TEXT_LENGTH = 500\n",
    "EMBED_SIZE  = 32\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "# Get the list of different classes\n",
    "CLASSES_LIST = np.unique(y_app)\n",
    "n_out = len(CLASSES_LIST)\n",
    "print(CLASSES_LIST)\n",
    "\n",
    "# Convert class string to index\n",
    "train_y_cat = np_utils.to_categorical(y_app, n_out)\n",
    "test_y_cat = np_utils.to_categorical(y_test, n_out)\n",
    "\n",
    "# get the textual data in the correct format for NN\n",
    "x_vec_train, x_vec_test = get_train_test(X_app, X_test)\n",
    "print(len(x_vec_train), len(x_vec_test))\n",
    "\n",
    "# define the NN topology\n",
    "model = get_model()\n",
    "\n",
    "# Create callbacks\n",
    "filepath = 'model_cnn' + '.hdf5'\n",
    "multi_checkpointer = ModelCheckpoint(filepath=filepath, verbose=0)\n",
    "multi_lr_reduction = ReduceLROnPlateau(monitor='val_loss', patience=5, verbose=0, factor=0.2)\n",
    "\n",
    "# Train \n",
    "model.fit(x_vec_train, train_y_cat,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=EPOCHS, verbose=1, validation_split=VALIDATION_SPLIT,callbacks=[multi_checkpointer,multi_lr_reduction])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7621776504297995\n",
      "p r f1 76.2 76.22 76.218\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.48      0.56        23\n",
      "          1       0.98      0.94      0.96        63\n",
      "          2       0.73      0.85      0.79        48\n",
      "          3       0.84      0.66      0.74        65\n",
      "          4       0.71      0.90      0.79        51\n",
      "          5       0.79      0.83      0.81        18\n",
      "          6       0.55      0.73      0.63        15\n",
      "          7       0.42      0.42      0.42        26\n",
      "          8       1.00      0.93      0.97        15\n",
      "          9       0.68      0.60      0.64        25\n",
      "\n",
      "avg / total       0.77      0.76      0.76       349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Matrice de probabilitées d'appartenance à chaque classe\n",
    "y_predicted = model.predict(x_vec_test)\n",
    "\n",
    "# Attribution de la classe qui a la plus probable\n",
    "y_pred = np.zeros(len(y_test))\n",
    "for i in range(len(y_predicted)):\n",
    "    y_pred[i] = np.argmax(y_predicted[i])\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "p, r, f1, s = precision_recall_fscore_support(y_test, y_pred, \n",
    "                                              average='micro',\n",
    "                                              labels=[x for x in \n",
    "                                                      np.unique(y_train) \n",
    "                                                      if x not in ['CSDECMOTV']])\n",
    "\n",
    "print('p r f1 %.1f %.2f %.3f' % (np.average(p, weights=s)*100.0, \n",
    "                                 np.average(r, weights=s)*100.0, \n",
    "                                 np.average(f1, weights=s)*100.0))\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred, labels=[x for x in \n",
    "                                                       np.unique(y_train) \n",
    "                                                       if x not in ['CSDECMOTV']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
